{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc67d376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269279e6",
   "metadata": {},
   "source": [
    "# Rock Paper Scissors Dataset Analysis\n",
    "This section loads and examines the rock-paper-scissors object detection dataset. We'll load the annotation data from a CSV file and perform initial data exploration to understand the structure and quality of our dataset.\n",
    "\n",
    "The dataset contains bounding box annotations for hand gestures representing rock, paper, and scissors poses, which will be used for training an object detection model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74bc63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image_info = pd.read_csv('./test/_annotations.csv')\n",
    "df_image_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35be4105",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9ba6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image_info.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8cbe06",
   "metadata": {},
   "source": [
    "## Data Quality Check\n",
    "\n",
    "Now let's examine the dataset for potential data quality issues, including duplicate values and other inconsistencies that could affect our model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c22efee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "duplicate_rows = df_image_info.duplicated()\n",
    "print(f\"Number of duplicate rows: {duplicate_rows.sum()}\")\n",
    "\n",
    "if duplicate_rows.sum() > 0:\n",
    "    print(\"\\nDuplicate rows found:\")\n",
    "    print(df_image_info[duplicate_rows])\n",
    "else:\n",
    "    print(\"No duplicate rows found.\")\n",
    "\n",
    "# Check for duplicate filenames\n",
    "duplicate_filenames = df_image_info['filename'].duplicated()\n",
    "print(f\"\\nNumber of duplicate filenames: {duplicate_filenames.sum()}\")\n",
    "\n",
    "if duplicate_filenames.sum() > 0:\n",
    "    print(\"\\nDuplicate filenames:\")\n",
    "    print(df_image_info[duplicate_filenames]['filename'].values)\n",
    "\n",
    "# Check for any missing values\n",
    "print(f\"\\nMissing values per column:\")\n",
    "print(df_image_info.isnull().sum())\n",
    "\n",
    "# Check for invalid bounding box coordinates\n",
    "invalid_boxes = df_image_info[(df_image_info['xmin'] >= df_image_info['xmax']) | \n",
    "                             (df_image_info['ymin'] >= df_image_info['ymax'])]\n",
    "print(f\"\\nNumber of invalid bounding boxes: {len(invalid_boxes)}\")\n",
    "\n",
    "if len(invalid_boxes) > 0:\n",
    "    print(\"Invalid bounding boxes found:\")\n",
    "    print(invalid_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da37082d",
   "metadata": {},
   "source": [
    "## Data Quality Analysis Results\n",
    " \n",
    "After examining the dataset for data quality issues, we found some interesting patterns:\n",
    " \n",
    " ### Duplicate Files Analysis\n",
    " The presence of duplicate filenames in our dataset is actually expected and intentional. \n",
    "These duplicates occur because some images contain multiple hands performing different gestures \n",
    "(e.g., one person showing \"Rock\" and another showing \"Paper\" in the same image). \n",
    " \n",
    "In such cases, the same image file is listed multiple times in our annotation data, each time with different bounding box coordinates corresponding to different hand regions. \n",
    "This is a common practice in object detection datasets where multiple objects of interest exist within a single image.\n",
    " \n",
    " ### Data Quality Summary\n",
    " - **Duplicate rows**: These represent legitimate multiple annotations for the same image\n",
    "- **Missing values**: None found in our dataset\n",
    " - **Bounding box validity**: All bounding boxes have valid coordinates (xmin < xmax, ymin < ymax)\n",
    " \n",
    " This dataset structure is appropriate for training an object detection model that can identify and localize multiple hand gestures within a single image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc1cebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all available styles\n",
    "print(plt.style.available)\n",
    "\n",
    "# Use a popular built-in style like 'ggplot' instead\n",
    "plt.style.use('_mpl-gallery')\n",
    "# Get the correct class counts\n",
    "class_counts = df_image_info['class'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(class_counts.index, class_counts.values, alpha=0.7)\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Rock, Paper, Scissors Classes')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122938f",
   "metadata": {},
   "source": [
    "## Sample Image Visualization\n",
    "\n",
    "The following section demonstrates how the bounding box annotations are applied to actual images in our dataset. We'll visualize the first image with its corresponding bounding box to verify the accuracy of our annotation data.\n",
    "\n",
    "This visualization helps us understand:\n",
    "- How well the bounding boxes align with the actual hand gestures\n",
    "- The quality of our annotation data\n",
    "- Whether any adjustments are needed before training our object detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2f3d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd()\n",
    "image_path = f\"./test/{df_image_info.loc[0, 'filename']}\"\n",
    "img = cv2.imread(image_path)\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load image\n",
    "img = cv2.imread(image_path)\n",
    "if img is not None:\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "else:\n",
    "    print(f\"Error: Could not load image from path: {image_path}\")\n",
    "    print(f\"Current directory: {current_path}\")\n",
    "    if os.path.exists(image_path):\n",
    "        print(\"File exists but couldn't be read by cv2.imread()\")\n",
    "    else:\n",
    "        print(\"File does not exist at the specified path\")\n",
    "\n",
    "# Load corresponding bounding boxes and labels\n",
    "# Get bounding box coordinates from the first row\n",
    "x1 = df_image_info.loc[0, 'xmin']\n",
    "y1 = df_image_info.loc[0, 'ymin']\n",
    "x2 = df_image_info.loc[0, 'xmax']\n",
    "y2 = df_image_info.loc[0, 'ymax']\n",
    "\n",
    "# Load image\n",
    "if img is not None:\n",
    "    # Load corresponding bounding boxes and labels\n",
    "    boxes = [[x1, y1, x2, y2]]  # Single bounding box for the first image\n",
    "    labels = [1]  # Assuming class labels: 1=Rock, 2=Paper, 3=Scissors\n",
    "\n",
    "    # Create a target dictionary\n",
    "    target = {}\n",
    "    target[\"boxes\"] = torch.tensor(boxes, dtype=torch.float32)\n",
    "    target[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "    # Visualize the image with bounding box\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(img)\n",
    "    plt.gca().add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, edgecolor='red', facecolor='none', linewidth=2))\n",
    "    plt.title(f\"Class: {df_image_info.loc[0, 'class']}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Target dictionary:\")\n",
    "    print(f\"Boxes shape: {target['boxes'].shape}\")\n",
    "    print(f\"Labels shape: {target['labels'].shape}\")\n",
    "    print(f\"Boxes: {target['boxes']}\")\n",
    "    print(f\"Labels: {target['labels']}\")\n",
    "\n",
    "    # Apply transform to the image\n",
    "    img_tensor = transform(img)\n",
    "\n",
    "    print(f\"Original image shape: {img.shape}\")\n",
    "    print(f\"Transformed image tensor shape: {img_tensor.shape}\")\n",
    "    print(f\"Tensor data type: {img_tensor.dtype}\")\n",
    "    print(f\"Tensor value range: [{img_tensor.min():.3f}, {img_tensor.max():.3f}]\")\n",
    "\n",
    "else:\n",
    "    print(f\"Error: Could not load image from path: {image_path}\")\n",
    "    print(f\"Current directory: {current_path}\")\n",
    "    if os.path.exists(image_path):\n",
    "        print(\"File exists but couldn't be read by cv2.imread()\")\n",
    "    else:\n",
    "        print(\"File does not exist at the specified path\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a0e646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_image_with_bbox(df, index=0, image_dir=\"./test/\"):\n",
    "    \"\"\"\n",
    "    Visualize an image with its bounding box annotation.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing image annotations\n",
    "        index: Row index to visualize (default: 0)\n",
    "        image_dir: Directory containing the images (default: \"./test/\")\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (img_tensor, target) if successful, (None, None) if failed\n",
    "    \"\"\"\n",
    "    # Get image path and load image\n",
    "    image_path = f\"{image_dir}{df.loc[index, 'filename']}\"\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    if img is None:\n",
    "        print(f\"Error: Could not load image from path: {image_path}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Convert BGR to RGB\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Get bounding box coordinates\n",
    "    x1 = df.loc[index, 'xmin']\n",
    "    y1 = df.loc[index, 'ymin']\n",
    "    x2 = df.loc[index, 'xmax']\n",
    "    y2 = df.loc[index, 'ymax']\n",
    "    \n",
    "    # Create target dictionary\n",
    "    boxes = [[x1, y1, x2, y2]]\n",
    "    labels = [1]  # Placeholder label\n",
    "    \n",
    "    target = {\n",
    "        \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
    "        \"labels\": torch.tensor(labels, dtype=torch.int64)\n",
    "    }\n",
    "    \n",
    "    # Visualize the image with bounding box\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(img)\n",
    "    plt.gca().add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, \n",
    "                                     edgecolor='red', facecolor='none', linewidth=2))\n",
    "    plt.title(f\"Class: {df.loc[index, 'class']}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # Apply transform to the image\n",
    "    img_tensor = transform(img)\n",
    "    \n",
    "    # Print information\n",
    "    print(f\"Target dictionary:\")\n",
    "    print(f\"Boxes shape: {target['boxes'].shape}\")\n",
    "    print(f\"Labels shape: {target['labels'].shape}\")\n",
    "    print(f\"Original image shape: {img.shape}\")\n",
    "    print(f\"Transformed image tensor shape: {img_tensor.shape}\")\n",
    "    \n",
    "    return img_tensor, target\n",
    "\n",
    "# Example usage\n",
    "# img_tensor, target = visualize_image_with_bbox(df_image_info, index=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2778616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store tensors and targets in lists\n",
    "image_tensors = []\n",
    "targets = []\n",
    "\n",
    "for i in range(len(df_image_info)):\n",
    "    img_tensor, target = visualize_image_with_bbox(df_image_info, index=i)\n",
    "    if img_tensor is not None:\n",
    "        image_tensors.append(img_tensor)\n",
    "        targets.append(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564892e9",
   "metadata": {},
   "source": [
    "## Model Setup\n",
    "\n",
    "Now we'll load and configure a pre-trained Faster R-CNN model for our rock-paper-scissors object detection task. We'll use a ResNet-50 backbone with Feature Pyramid Network (FPN) and modify the classifier head to match our dataset's classes.\n",
    "\n",
    "The model will be configured for 4 classes total:\n",
    "- Background (class 0) - automatically handled by the framework\n",
    "- Rock (class 1)\n",
    "- Paper (class 2) \n",
    "- Scissors (class 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b28ed18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained Faster R-CNN model with a ResNet-50 backbone\n",
    "model = fasterrcnn_resnet50_fpn(weights='DEFAULT')\n",
    "\n",
    "num_classes = 4  # 3 classes (Rock, Paper, Scissors) + background\n",
    "\n",
    "# Get the number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# Replace the head of the model with a new one (for the number of classes in your dataset)\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb57c9ea",
   "metadata": {},
   "source": [
    "## Data Loader Setup\n",
    "\n",
    "Now we'll set up the data loading pipeline for our rock-paper-scissors dataset. This involves creating data loaders that will efficiently batch and serve our training data to the model during training and validation phases.\n",
    "\n",
    "The data loaders will handle proper batching, shuffling, and collation of our image tensors and target annotations to ensure smooth training workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94246bea",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc38d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to tensors and create dataset\n",
    "# Don't stack image_tensors since they're already individual tensors in a list\n",
    "# targets = targets  # This line is redundant, removing it\n",
    "\n",
    "# Import required modules\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Create a simple dataset from our tensors\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, images, targets):\n",
    "        self.images = images\n",
    "        self.targets = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.targets[idx]\n",
    "\n",
    "# Load dataset\n",
    "dataset = SimpleDataset(image_tensors, targets)\n",
    "\n",
    "# Split into train and validation sets\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "train_dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "valid_dataset = torch.utils.data.Subset(dataset, indices[-50:])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, \n",
    "                         collate_fn=lambda x: tuple(zip(*x)))\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=4, shuffle=False, \n",
    "                         collate_fn=lambda x: tuple(zip(*x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecb50d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, data_loader, device):\n",
    "    \"\"\"Train model for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for images, targets in data_loader:\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += losses.item()\n",
    "    \n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384dc7c6",
   "metadata": {},
   "source": [
    "## Setup Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aeff088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, data_loader, device):\n",
    "    # It's good practice to set eval mode at the beginning.\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in data_loader:\n",
    "            images = [image.to(device) for image in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            # Temporarily set the model to training mode to get the loss dictionary.\n",
    "            # Because we are in a torch.no_grad() context, no gradients will be computed.\n",
    "            model.train()\n",
    "            loss_dict = model(images, targets)\n",
    "            \n",
    "            # Switch back to eval mode for the next batch or subsequent evaluations.\n",
    "            model.eval()\n",
    "\n",
    "            # The model in train() mode returns a dictionary of losses.\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            total_loss += losses.item()\n",
    "    \n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# The rest of your training loop code remains the same.\n",
    "# Training parameters\n",
    "num_epochs = 10\n",
    "learning_rate = 0.005\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "# Training loop\n",
    "print(f\"Training on device: {device}\")\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(model, optimizer, train_loader, device)\n",
    "    # Now, this call will work correctly.\n",
    "    val_loss = validate(model, valid_loader, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Training Loss: {train_loss:.4f}\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3f544d",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1e8b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model using joblib\n",
    "import joblib\n",
    "\n",
    "# Save the model to a file\n",
    "model_filename = 'trained_model.joblib'\n",
    "joblib.dump(model, model_filename)\n",
    "print(f\"Model saved to {model_filename}\")\n",
    "\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Lists to store predictions and ground truth\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "\n",
    "# Evaluate on validation set\n",
    "with torch.no_grad():\n",
    "    for images, targets in valid_loader:\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = model(images)\n",
    "        \n",
    "        # Store predictions and targets\n",
    "        all_predictions.extend(predictions)\n",
    "        all_targets.extend(targets)\n",
    "\n",
    "# Calculate metrics\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"Calculate Intersection over Union between two bounding boxes\"\"\"\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    \n",
    "    intersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union = box1_area + box2_area - intersection\n",
    "    \n",
    "    return intersection / union if union > 0 else 0\n",
    "\n",
    "# Calculate mAP and IoU metrics\n",
    "iou_thresholds = [0.5, 0.75]\n",
    "class_names = ['Rock', 'Paper', 'Scissors']\n",
    "metrics = defaultdict(list)\n",
    "\n",
    "for pred, target in zip(all_predictions, all_targets):\n",
    "    pred_boxes = pred['boxes'].cpu().numpy()\n",
    "    pred_scores = pred['scores'].cpu().numpy()\n",
    "    pred_labels = pred['labels'].cpu().numpy()\n",
    "    \n",
    "    target_boxes = target['boxes'].cpu().numpy()\n",
    "    target_labels = target['labels'].cpu().numpy()\n",
    "    \n",
    "    # Calculate IoU for each prediction\n",
    "    for i, (pred_box, pred_score, pred_label) in enumerate(zip(pred_boxes, pred_scores, pred_labels)):\n",
    "        best_iou = 0\n",
    "        best_match = None\n",
    "        \n",
    "        for j, (target_box, target_label) in enumerate(zip(target_boxes, target_labels)):\n",
    "            if pred_label == target_label:\n",
    "                iou = calculate_iou(pred_box, target_box)\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_match = j\n",
    "        \n",
    "        # Check if prediction is correct at different IoU thresholds\n",
    "        for threshold in iou_thresholds:\n",
    "            if best_iou >= threshold:\n",
    "                metrics[f'correct_{threshold}'].append(1)\n",
    "            else:\n",
    "                metrics[f'correct_{threshold}'].append(0)\n",
    "        \n",
    "        metrics['iou_scores'].append(best_iou)\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Model Evaluation Results:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Average IoU\n",
    "avg_iou = np.mean(metrics['iou_scores'])\n",
    "print(f\"Average IoU: {avg_iou:.4f}\")\n",
    "\n",
    "# Precision at different IoU thresholds\n",
    "for threshold in iou_thresholds:\n",
    "    precision = np.mean(metrics[f'correct_{threshold}'])\n",
    "    print(f\"Precision at IoU {threshold}: {precision:.4f}\")\n",
    "\n",
    "# Per-class metrics\n",
    "print(\"\\nPer-class metrics:\")\n",
    "for class_name in class_names:\n",
    "    class_ious = [iou for iou, label in zip(metrics['iou_scores'], pred_labels) \n",
    "                  if class_names[label] == class_name]\n",
    "    if class_ious:\n",
    "        avg_class_iou = np.mean(class_ious)\n",
    "        print(f\"{class_name}: Average IoU = {avg_class_iou:.4f}\")\n",
    "\n",
    "# Visualize some predictions\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def visualize_predictions(model, data_loader, num_samples=5):\n",
    "    \"\"\"Visualize model predictions on sample images\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(20, 4))\n",
    "    if num_samples == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, (images, targets) in enumerate(data_loader):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "            \n",
    "        image = images[0]\n",
    "        target = targets[0]\n",
    "        \n",
    "        # Get prediction\n",
    "        with torch.no_grad():\n",
    "            prediction = model([image.to(device)])[0]\n",
    "        \n",
    "        # Convert to numpy for visualization\n",
    "        img_np = image.permute(1, 2, 0).numpy()\n",
    "        \n",
    "        # Plot image\n",
    "        ax = axes[i]\n",
    "        ax.imshow(img_np)\n",
    "        ax.set_title(f'Sample {i+1}')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Draw ground truth boxes (green)\n",
    "        for box, label in zip(target['boxes'], target['labels']):\n",
    "            x1, y1, x2, y2 = box.numpy()\n",
    "            rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                                   linewidth=2, edgecolor='green', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(x1, y1-5, class_names[label], color='green', fontsize=8)\n",
    "        \n",
    "        # Draw prediction boxes (red)\n",
    "        for box, score, label in zip(prediction['boxes'], prediction['scores'], prediction['labels']):\n",
    "            if score > 0.5:  # Only show high confidence predictions\n",
    "                x1, y1, x2, y2 = box.cpu().numpy()\n",
    "                rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                                       linewidth=2, edgecolor='red', facecolor='none')\n",
    "                ax.add_patch(rect)\n",
    "                ax.text(x1, y1-10, f\"{class_names[label]} ({score:.2f})\", \n",
    "                       color='red', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize predictions\n",
    "print(\"\\nVisualizing sample predictions...\")\n",
    "visualize_predictions(model, valid_loader, num_samples=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebd840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "from torchvision import transforms\n",
    "\n",
    "def get_transform(train):\n",
    "    \"\"\"Get image transforms for training or validation\"\"\"\n",
    "    transform_list = [\n",
    "        transforms.Resize((800, 800)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    "    if train:\n",
    "        transform_list.append(transforms.RandomHorizontalFlip(0.5))\n",
    "    return transforms.Compose(transform_list)\n",
    "\n",
    "def webcam_detection(model, device, class_names):\n",
    "    \"\"\"\n",
    "    Real-time object detection using webcam\n",
    "    \"\"\"\n",
    "    # Initialize webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam\")\n",
    "        return\n",
    "    \n",
    "    print(\"Webcam started. Press 'q' to quit.\")\n",
    "    \n",
    "    while True:\n",
    "        # Capture frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Could not read frame\")\n",
    "            break\n",
    "        \n",
    "        # Convert BGR to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Convert to PIL Image\n",
    "        pil_image = Image.fromarray(frame_rgb)\n",
    "        \n",
    "        # Transform image\n",
    "        transform = get_transform(train=False)\n",
    "        image_tensor = transform(pil_image).unsqueeze(0)\n",
    "        \n",
    "        # Get prediction\n",
    "        with torch.no_grad():\n",
    "            prediction = model([image_tensor.to(device)])[0]\n",
    "        \n",
    "        # Draw predictions on frame\n",
    "        for box, score, label in zip(prediction['boxes'], prediction['scores'], prediction['labels']):\n",
    "            if score > 0.5:  # Only show high confidence predictions\n",
    "                x1, y1, x2, y2 = box.cpu().numpy().astype(int)\n",
    "                \n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                \n",
    "                # Add label and confidence\n",
    "                label_text = f\"{class_names[label]} ({score:.2f})\"\n",
    "                cv2.putText(frame, label_text, (x1, y1-10), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        \n",
    "        # Display frame\n",
    "        cv2.imshow('Rock Paper Scissors Detection', frame)\n",
    "        \n",
    "        # Break loop on 'q' press\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Start real-time detection\n",
    "print(\"Starting real-time webcam detection...\")\n",
    "webcam_detection(model, device, class_names)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pedra-papel-tesoura-FeOq4tIs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
